{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"D:\\\\projects\\\\v2v\\\\v5\\\\data\\\\audio_list.json\", \"r\") as f:\n",
    "    audio_list = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:02<00:00, 149.49it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "base_dir = \"F:/video_classification\"\n",
    "video_dir = [f\"{base_dir}/{i}\" for i in os.listdir(base_dir)]\n",
    "\n",
    "audio_list = []\n",
    "for item in tqdm(video_dir):#video_dir:\n",
    "    chunk_dir = [f\"{item}/{i}\" for i in os.listdir(item) if os.path.isdir(f\"{item}/{i}\")]\n",
    "    for chunk in chunk_dir:\n",
    "        audio_dir = [f\"{chunk}/{i}\" for i in os.listdir(chunk) if i.startswith(\"video\")]\n",
    "        audio_list.append(audio_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 341.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files: 27414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import itertools\n",
    "\n",
    "base_dir = \"F:/video_classification\"\n",
    "\n",
    "# Dùng os.scandir() để tăng tốc duyệt thư mục\n",
    "video_dirs = [entry.path for entry in os.scandir(base_dir) if entry.is_dir()]\n",
    "\n",
    "def process_chunk_dir(item):\n",
    "    \"\"\"Xử lý từng thư mục chunk, trả về danh sách audio file\"\"\"\n",
    "    chunk_dirs = [entry.path for entry in os.scandir(item) if entry.is_dir()]\n",
    "    audio_files = list(itertools.chain.from_iterable(\n",
    "        [[f\"{chunk}/{i}\" for i in os.listdir(chunk) if i.startswith(\"video\")]\n",
    "         for chunk in chunk_dirs]\n",
    "    ))\n",
    "    return audio_files\n",
    "\n",
    "# Sử dụng ThreadPoolExecutor để đa luồng\n",
    "audio_list = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(tqdm(executor.map(process_chunk_dir, video_dirs), total=len(video_dirs)))\n",
    "\n",
    "# Flatten danh sách kết quả\n",
    "audio_list = list(itertools.chain.from_iterable(results))\n",
    "\n",
    "print(f\"Total audio files: {len(audio_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "# Path to your JSON file\n",
    "json_file_path = \"D:\\\\projects\\\\v2v\\\\v5\\\\audio_list.json\"\n",
    "\n",
    "# Open and load JSON file\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    file_paths = json.load(f)  # Convert JSON to Python object (dict or list)\n",
    "\n",
    "# Nhóm video theo trận đấu\n",
    "video_groups = defaultdict(list)\n",
    "\n",
    "for path in file_paths:\n",
    "    match = re.search(r'video_classification/(.*?)/chunk_\\d+', path)\n",
    "    if match:\n",
    "        match_key = match.group(1)  # Lấy phần \"2014-11-04 - 22-45 Arsenal 3 - 3 Anderlecht\"\n",
    "        video_groups[match_key].append(path)\n",
    "\n",
    "# Hàm trích xuất số chunk để sắp xếp\n",
    "def extract_chunk_number(path):\n",
    "    match = re.search(r'chunk_(\\d+)', path)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "# Sắp xếp theo trận đấu và số chunk\n",
    "sorted_videos = []\n",
    "for match_key in sorted(video_groups.keys()):  # Sắp xếp theo trận đấu trước\n",
    "    sorted_videos.extend(sorted(video_groups[match_key], key=extract_chunk_number))\n",
    "\n",
    "# In kết quả\n",
    "# print(\"\\n\".join(sorted_videos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_videos = sorted_videos[13196:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27414/27414 [00:00<00:00, 197269.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "def normalize_path(path: str):\n",
    "    return Path(path).as_posix()\n",
    "\n",
    "audio_lists = []\n",
    "for item in tqdm(audio_list):\n",
    "    audio_lists.append(normalize_path(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Danh sách audio đã được lưu vào: audio_list.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Đường dẫn lưu tệp JSON\n",
    "json_file_path = \"audio_list.json\"\n",
    "\n",
    "# Ghi danh sách vào tệp JSON\n",
    "with open(json_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(audio_lists, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Danh sách audio đã được lưu vào: {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "audio_list = pandas.read_csv(\"dataset2.csv\", sep = \";\")\n",
    "audio_list = audio_list['audio_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zul_Latn'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGUAGE_CODE = {\n",
    "    'ace': 'ace_Latn',\n",
    "    'ar': 'arz_Arab',\n",
    "    'af': 'afr_Latn',\n",
    "    'ak': 'aka_Latn',\n",
    "    'am': 'amh_Ethi',\n",
    "    'as': 'asm_Beng',\n",
    "    'ast': 'ast_Latn',\n",
    "    'awa': 'awa_Deva',\n",
    "    'ay': 'ayr_Latn',\n",
    "    'az': 'azj_Latn',\n",
    "    'ba': 'bak_Cyrl',\n",
    "    'bm': 'bam_Latn',\n",
    "    'ban': 'ban_Latn',\n",
    "    'be': 'bel_Cyrl',\n",
    "    'bem': 'bem_Latn',\n",
    "    'bn': 'ben_Beng',\n",
    "    'bho': 'bho_Deva',\n",
    "    'bjn': 'bjn_Latn',\n",
    "    'bo': 'bod_Tibt',\n",
    "    'bs': 'bos_Latn',\n",
    "    'bug': 'bug_Latn',\n",
    "    'bg': 'bul_Cyrl',\n",
    "    'ca': 'cat_Latn',\n",
    "    'ceb': 'ceb_Latn',\n",
    "    'cs': 'ces_Latn',\n",
    "    'cjk': 'cjk_Latn',\n",
    "    'ckb': 'ckb_Arab',\n",
    "    'crh': 'crh_Latn',\n",
    "    'cy': 'cym_Latn',\n",
    "    'da': 'dan_Latn',\n",
    "    'de': 'deu_Latn',\n",
    "    'dik': 'dik_Latn',\n",
    "    'dyu': 'dyu_Latn',\n",
    "    'dz': 'dzo_Tibt',\n",
    "    'el': 'ell_Grek',\n",
    "    'en': 'eng_Latn',\n",
    "    'eo': 'epo_Latn',\n",
    "    'et': 'est_Latn',\n",
    "    'eu': 'eus_Latn',\n",
    "    'ewe': 'ewe_Latn',\n",
    "    'fo': 'fao_Latn',\n",
    "    'fa': 'pes_Arab',\n",
    "    'fij': 'fij_Latn',\n",
    "    'fi': 'fin_Latn',\n",
    "    'fon': 'fon_Latn',\n",
    "    'fr': 'fra_Latn',\n",
    "    'fur': 'fur_Latn',\n",
    "    'fuv': 'fuv_Latn',\n",
    "    'gd': 'gla_Latn',\n",
    "    'ga': 'gle_Latn',\n",
    "    'gl': 'glg_Latn',\n",
    "    'gn': 'grn_Latn',\n",
    "    'gu': 'guj_Gujr',\n",
    "    'ht': 'hat_Latn',\n",
    "    'ha': 'hau_Latn',\n",
    "    'he': 'heb_Hebr',\n",
    "    'hi': 'hin_Deva',\n",
    "    'hne': 'hne_Deva',\n",
    "    'hr': 'hrv_Latn',\n",
    "    'hu': 'hun_Latn',\n",
    "    'hy': 'hye_Armn',\n",
    "    'ig': 'ibo_Latn',\n",
    "    'ilo': 'ilo_Latn',\n",
    "    'id': 'ind_Latn',\n",
    "    'is': 'isl_Latn',\n",
    "    'it': 'ita_Latn',\n",
    "    'jv': 'jav_Latn',\n",
    "    'ja': 'jpn_Jpan',\n",
    "    'kab': 'kab_Latn',\n",
    "    'kac': 'kac_Latn',\n",
    "    'kam': 'kam_Latn',\n",
    "    'kn': 'kan_Knda',\n",
    "    'ks': 'kas_Deva',\n",
    "    'ka': 'kat_Geor',\n",
    "    'knc': 'knc_Latn',\n",
    "    'kk': 'kaz_Cyrl',\n",
    "    'kbp': 'kbp_Latn',\n",
    "    'kea': 'kea_Latn',\n",
    "    'km': 'khm_Khmr',\n",
    "    'ki': 'kik_Latn',\n",
    "    'rw': 'kin_Latn',\n",
    "    'ky': 'kir_Cyrl',\n",
    "    'kmb': 'kmb_Latn',\n",
    "    'kon': 'kon_Latn',\n",
    "    'ko': 'kor_Hang',\n",
    "    'kmr': 'kmr_Latn',\n",
    "    'lo': 'lao_Laoo',\n",
    "    'lv': 'lvs_Latn',\n",
    "    'lij': 'lij_Latn',\n",
    "    'li': 'lim_Latn',\n",
    "    'ln': 'lin_Latn',\n",
    "    'lt': 'lit_Latn',\n",
    "    'lmo': 'lmo_Latn',\n",
    "    'ltg': 'ltg_Latn',\n",
    "    'lb': 'ltz_Latn',\n",
    "    'lua': 'lua_Latn',\n",
    "    'lg': 'lug_Latn',\n",
    "    'luo': 'luo_Latn',\n",
    "    'lus': 'lus_Latn',\n",
    "    'mag': 'mag_Deva',\n",
    "    'mai': 'mai_Deva',\n",
    "    'ml': 'mal_Mlym',\n",
    "    'mr': 'mar_Deva',\n",
    "    'min': 'min_Latn',\n",
    "    'mk': 'mkd_Cyrl',\n",
    "    'plt': 'plt_Latn',\n",
    "    'mt': 'mlt_Latn',\n",
    "    'mni': 'mni_Beng',\n",
    "    'khk': 'khk_Cyrl',\n",
    "    'mos': 'mos_Latn',\n",
    "    'mi': 'mri_Latn',\n",
    "    'ms': 'zsm_Latn',\n",
    "    'my': 'mya_Mymr',\n",
    "    'nl': 'nld_Latn',\n",
    "    'nn': 'nno_Latn',\n",
    "    'nb': 'nob_Latn',\n",
    "    'np': 'npi_Deva',\n",
    "    'nso': 'nso_Latn',\n",
    "    'nus': 'nus_Latn',\n",
    "    'ny': 'nya_Latn',\n",
    "    'oc': 'oci_Latn',\n",
    "    'gaz': 'gaz_Latn',\n",
    "    'or': 'ory_Orya',\n",
    "    'pag': 'pag_Latn',\n",
    "    'pa': 'pan_Guru',\n",
    "    'pap': 'pap_Latn',\n",
    "    'pl': 'pol_Latn',\n",
    "    'pt': 'por_Latn',\n",
    "    'prs': 'prs_Arab',\n",
    "    'pbt': 'pbt_Arab',\n",
    "    'quy': 'quy_Latn',\n",
    "    'ro': 'ron_Latn',\n",
    "    'rn': 'run_Latn',\n",
    "    'ru': 'rus_Cyrl',\n",
    "    'sag': 'sag_Latn',\n",
    "    'sa': 'san_Deva',\n",
    "    'sat': 'sat_Beng',\n",
    "    'scn': 'scn_Latn',\n",
    "    'shn': 'shn_Mymr',\n",
    "    'si': 'sin_Sinh',\n",
    "    'sk': 'slk_Latn',\n",
    "    'sl': 'slv_Latn',\n",
    "    'sm': 'smo_Latn',\n",
    "    'sn': 'sna_Latn',\n",
    "    'sd': 'snd_Arab',\n",
    "    'so': 'som_Latn',\n",
    "    'st': 'sot_Latn',\n",
    "    'es': 'spa_Latn',\n",
    "    'sq': 'als_Latn',\n",
    "    'sc': 'srd_Latn',\n",
    "    'sr': 'srp_Cyrl',\n",
    "    'ss': 'ssw_Latn',\n",
    "    'su': 'sun_Latn',\n",
    "    'sv': 'swe_Latn',\n",
    "    'sw': 'swh_Latn',\n",
    "    'szl': 'szl_Latn',\n",
    "    'ta': 'tam_Taml',\n",
    "    'tt': 'tat_Cyrl',\n",
    "    'te': 'tel_Telu',\n",
    "    'tg': 'tgk_Cyrl',\n",
    "    'tl': 'tgl_Latn',\n",
    "    'th': 'tha_Thai',\n",
    "    'ti': 'tir_Ethi',\n",
    "    'taq': 'taq_Tfng',\n",
    "    'tpi': 'tpi_Latn',\n",
    "    'tsn': 'tsn_Latn',\n",
    "    'tso': 'tso_Latn',\n",
    "    'tk': 'tuk_Latn',\n",
    "    'tum': 'tum_Latn',\n",
    "    'tr': 'tur_Latn',\n",
    "    'tw': 'twi_Latn',\n",
    "    'tzm': 'tzm_Tfng',\n",
    "    'ug': 'uig_Arab',\n",
    "    'uk': 'ukr_Cyrl',\n",
    "    'umb': 'umb_Latn',\n",
    "    'ur': 'urd_Arab',\n",
    "    'uz': 'uzn_Latn',\n",
    "    'vec': 'vec_Latn',\n",
    "    'vi': 'vie_Latn',\n",
    "    'war': 'war_Latn',\n",
    "    'wo': 'wol_Latn',\n",
    "    'xh': 'xho_Latn',\n",
    "    'yid': 'ydd_Hebr',\n",
    "    'yo': 'yor_Latn',\n",
    "    'yue': 'yue_Hant',\n",
    "    'zh': 'zho_Hant',\n",
    "    'zu': 'zul_Latn'}\n",
    "\n",
    "LANGUAGE_CODE.get('zu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\v2v\\v5\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def process_file_with_run(args):\n",
    "    \"\"\"\n",
    "    Hàm độc lập để xử lý một file âm thanh sử dụng phương thức run()\n",
    "    \"\"\"\n",
    "    audio_path, config = args\n",
    "    \n",
    "    # Tạo một instance Audio2Text mới với file đầu ra tạm thời\n",
    "    processor = Audio2Text(\n",
    "        stt_model_name=config['stt_model_name'],\n",
    "        translate_model_name=config['translate_model_name'],\n",
    "        max_text_gen=config['max_text_gen'],\n",
    "        idx=config['idx'],\n",
    "        device=config['device'],\n",
    "        cpu_threads=config['cpu_threads'],\n",
    "        compute_type=config['compute_type'],\n",
    "        src_lang=config['src_lang'],\n",
    "        target_lang=config['target_lang'],\n",
    "        json_output_dir=config['temp_output_dir']\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Xử lý file và lưu vào file tạm\n",
    "        result = processor.run(audio_path)\n",
    "        return {\n",
    "            'success': True,\n",
    "            'result': result,\n",
    "            'audio_path': audio_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'audio_path': audio_path\n",
    "        }\n",
    "class Audio2Text:\n",
    "    def __init__(self, \n",
    "                 stt_model_name: str = 'distil-large-v3',\n",
    "                 translate_model_name: str = \"Emilio407/nllb-200-3.3B-8bit\", #\"facebook/nllb-200-3.3B\",\n",
    "                 max_text_gen: int = 625,\n",
    "                 idx: int = 0,\n",
    "                 device: Optional[str] = None,\n",
    "                 cpu_threads: int = 4,\n",
    "                 compute_type: str = 'int8_float16',\n",
    "                 src_lang: str = None,\n",
    "                 target_lang: str = 'eng_Latn',\n",
    "                 json_output_dir: str = 'translated_audio2text.jsonl'):\n",
    "        super(Audio2Text, self).__init__()\n",
    "        self.stt_model_name = stt_model_name\n",
    "        self.translate_model_name = translate_model_name\n",
    "        self.max_text_gen = max_text_gen\n",
    "        \n",
    "        self.idx = idx\n",
    "        \n",
    "        if device is None:\n",
    "            os.environ['CUDA_VISIBLE_DEVICES'] = str(self.idx)\n",
    "            self.device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            \n",
    "        self.cpu_threads = cpu_threads\n",
    "        self.compute_type = compute_type\n",
    "        self.json_output_dir = json_output_dir\n",
    "        \n",
    "        # Khởi tạo mô hình faster-whisper\n",
    "        if self.stt_model_name is not None:\n",
    "            self.stt_model = WhisperModel(model_size_or_path = self.stt_model_name, \n",
    "                             device=self.device,\n",
    "                             device_index= self.idx, \n",
    "                             cpu_threads=self.cpu_threads,\n",
    "                             compute_type=self.compute_type,\n",
    "                             )\n",
    "        else:\n",
    "            raise Exception(\"Model For Speech To text not found. Please add a valid model name.\")\n",
    "        \n",
    "        # khởi tạo mô hình dịch\n",
    "        if self.translate_model_name is not None:\n",
    "            self.translate_model = AutoModelForSeq2SeqLM.from_pretrained(self.translate_model_name)\n",
    "            self.translate_tokenizer = AutoTokenizer.from_pretrained(self.translate_model_name)\n",
    "        else:\n",
    "            raise Exception(\"Model for Translation not found. Please add a valid model name.\")\n",
    "        \n",
    "        self.src_lang = src_lang \n",
    "        self.target_lang = target_lang\n",
    "        self.translator = None\n",
    "        \n",
    "        # Chỉ khởi tạo translator nếu src_lang đã được xác định\n",
    "        if self.src_lang is not None:\n",
    "            self.create_translator(self.src_lang, self.target_lang)\n",
    "    \n",
    "    def create_translator(self, src_lang, tgt_lang):\n",
    "        \"\"\"\n",
    "        Tạo hoặc cập nhật translator với ngôn ngữ nguồn và đích chỉ định\n",
    "        \"\"\"\n",
    "        self.translator = pipeline('translation', \n",
    "                            model=self.translate_model_name, \n",
    "                            tokenizer=self.translate_tokenizer, \n",
    "                            src_lang=src_lang, \n",
    "                            tgt_lang=tgt_lang, \n",
    "                            device=self.idx,\n",
    "                            max_length=self.max_text_gen,\n",
    "                            use_fast=True)\n",
    "        return self.translator\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_sentence_with_phrase(text, phrase):\n",
    "        sentences = text.split('.')\n",
    "        filtered_sentences = [sentence.strip() for sentence in sentences if phrase.lower() not in sentence.lower()]\n",
    "        filtered_sentences =  '. '.join(filtered_sentences) + ('' if len(filtered_sentences) > 0 else '')\n",
    "        return filtered_sentences.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def postprocess_translation(text: str):\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        text = text.replace(\"No.\", \"the number\")\n",
    "        text = Audio2Text.remove_sentence_with_phrase(text, \"Please subcribe\")\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        text = text.replace(\". . .\", \".\")\n",
    "        text = text.replace (\"...\", \".\")\n",
    "        return text\n",
    "        \n",
    "    def transcribe(self, audio_path):\n",
    "        segments, info = self.stt_model.transcribe(audio_path, multilingual= True,\n",
    "                                                beam_size= 3,\n",
    "                                                no_speech_threshold = 1.0, \n",
    "                                                hotwords =\"pressure, penalty, strike, shot, pass, foul, goal, offside, corner, save, clearance, counter\",\n",
    "                                                vad_filter = True,\n",
    "                                                vad_parameters=dict(min_silence_duration_ms=1000))\n",
    "        text = \" \".join([segment.text for segment in segments])\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        return text, info.language, info.language_probability\n",
    "    \n",
    "    def __google_translate(self, text):\n",
    "        translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
    "        text = translator.translate(text)\n",
    "        text = Audio2Text.postprocess_translation(text)\n",
    "        return text.replace(\"  \", \" \")\n",
    "    \n",
    "    def save_to_jsonl(self, result_dict):\n",
    "        \"\"\"\n",
    "        Save translation result to JSONL file (JSON Lines)\n",
    "        Each result is written as a separate line in the file\n",
    "        \"\"\"\n",
    "        # Append new result as a new line\n",
    "        with open(self.json_output_dir, 'a', encoding='utf-8') as f:\n",
    "            # If the file is new/empty, no need for a newline at the beginning\n",
    "            if os.path.getsize(self.json_output_dir) > 0:\n",
    "                f.write('\\n')\n",
    "            f.write(json.dumps(result_dict, ensure_ascii=False))\n",
    "    \n",
    "    def translation(self, audio_path: str):\n",
    "        \"\"\"\n",
    "        - Đầu tiên sử dụng transcribe của fasterwhisper để dịch\n",
    "        - Check xem language là gì, để convert sang tiếng anh\n",
    "        - Nếu language không phải là tiếng anh:\n",
    "            - thực hiện translation và nếu translation bị none thì sử dụng google translatetranslate\n",
    "        \"\"\"\n",
    "        \n",
    "        text, lang, lang_prob = self.transcribe(audio_path)\n",
    "        origin_text = text\n",
    "        translation_type = \"no_translation\"\n",
    "        translated_text = text\n",
    "\n",
    "        if lang != 'en' or lang_prob <= 0.8:\n",
    "            detected_src_lang = None\n",
    "            \n",
    "            # Xác định ngôn ngữ nguồn\n",
    "            if self.src_lang is None:\n",
    "                try:\n",
    "                    detected_src_lang = LANGUAGE_CODE.get(lang)\n",
    "                    if detected_src_lang is None:\n",
    "                        # Không tìm thấy mã ngôn ngữ, dùng Google Translate\n",
    "                        translated_text = self.__google_translate(text)\n",
    "                        translation_type = \"deep\"\n",
    "                        return {\n",
    "                            \"audio_path\": audio_path,\n",
    "                            \"translation_type\": translation_type,\n",
    "                            \"origin\": origin_text,\n",
    "                            \"translation\": translated_text\n",
    "                        }\n",
    "                except Exception:\n",
    "                    # Lỗi khi xác định ngôn ngữ, dùng Google Translate\n",
    "                    translated_text = self.__google_translate(text)\n",
    "                    translation_type = \"deep\"\n",
    "                    return {\n",
    "                        \"audio_path\": audio_path,\n",
    "                        \"translation_type\": translation_type,\n",
    "                        \"origin\": origin_text,\n",
    "                        \"translation\": translated_text\n",
    "                    }\n",
    "            else:\n",
    "                detected_src_lang = self.src_lang\n",
    "            \n",
    "            try:\n",
    "                # Tạo translator nếu cần\n",
    "                if self.translator is None or (detected_src_lang and detected_src_lang != self.src_lang):\n",
    "                    self.create_translator(detected_src_lang, self.target_lang)\n",
    "                \n",
    "                # Dịch văn bản\n",
    "                result = self.translator(text)\n",
    "                translated_text = result[0]['translation_text']\n",
    "                translation_type = \"nlln\"\n",
    "                \n",
    "                if translated_text is None or translated_text == \" \" or translated_text == \"\":\n",
    "                    translated_text = self.__google_translate(text)\n",
    "                    translation_type = \"deep\"\n",
    "            except Exception:\n",
    "                # Fallback to Google Translate if any error occurs\n",
    "                translated_text = self.__google_translate(text)\n",
    "                translation_type = \"deep\"\n",
    "        \n",
    "        result = {\n",
    "            \"audio_path\": audio_path,\n",
    "            \"translation_type\": translation_type, \n",
    "            \"origin\": origin_text,\n",
    "            \"translation\": translated_text\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    def run(self, audio_path: str,):\n",
    "        \"\"\"\n",
    "        Complete process: transcribe, translate, and save results to JSON in one step\n",
    "        \n",
    "        Args:\n",
    "            src_lang: Source language code (optional)\n",
    "            target_lang: Target language code, defaults to 'eng_Latn'\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing translation results\n",
    "        \"\"\"\n",
    "        # Get translation results\n",
    "        result = self.translation(audio_path = audio_path)\n",
    "        \n",
    "        # Save results to JSON\n",
    "        self.save_to_jsonl(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_translation(self, texts: List[str], langs: List[str], audio_paths: List[str]):\n",
    "        \"\"\"\n",
    "        Dịch nhiều văn bản cùng lúc để tận dụng khả năng xử lý batch của GPU\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Xác định các văn bản cần dịch (không phải tiếng Anh)\n",
    "        texts_by_lang = {}  # Nhóm văn bản theo ngôn ngữ\n",
    "        \n",
    "        for i, (text, lang, path) in enumerate(zip(texts, langs, audio_paths)):\n",
    "            if lang == 'en':\n",
    "                # Không cần dịch văn bản tiếng Anh\n",
    "                results.append({\n",
    "                    \"audio_path\": path,\n",
    "                    \"translation_type\": \"no_translation\",\n",
    "                    \"origin\": text,\n",
    "                    \"translation\": text\n",
    "                })\n",
    "            else:\n",
    "                # Xác định mã ngôn ngữ nguồn\n",
    "                try:\n",
    "                    src_lang_code = self.src_lang if self.src_lang else LANGUAGE_CODE.get(lang)\n",
    "                    \n",
    "                    if src_lang_code:\n",
    "                        # Nhóm theo ngôn ngữ\n",
    "                        if src_lang_code not in texts_by_lang:\n",
    "                            texts_by_lang[src_lang_code] = []\n",
    "                        \n",
    "                        texts_by_lang[src_lang_code].append({\n",
    "                            \"index\": i,\n",
    "                            \"text\": text,\n",
    "                            \"path\": path\n",
    "                        })\n",
    "                    else:\n",
    "                        # Không tìm thấy mã ngôn ngữ, dùng Google Translate\n",
    "                        translated = self.__google_translate(text)\n",
    "                        results.append({\n",
    "                            \"audio_path\": path,\n",
    "                            \"translation_type\": \"deep\",\n",
    "                            \"origin\": text,\n",
    "                            \"translation\": translated\n",
    "                        })\n",
    "                except Exception:\n",
    "                    # Xử lý lỗi, dùng Google Translate\n",
    "                    translated = self.__google_translate(text)\n",
    "                    results.append({\n",
    "                        \"audio_path\": path,\n",
    "                        \"translation_type\": \"deep\",\n",
    "                        \"origin\": text,\n",
    "                        \"translation\": translated\n",
    "                    })\n",
    "        \n",
    "        # Xử lý dịch theo từng ngôn ngữ\n",
    "        for src_lang_code, text_group in texts_by_lang.items():\n",
    "            try:\n",
    "                # Tạo translator cho ngôn ngữ này\n",
    "                translator = self.create_translator(src_lang_code, self.target_lang)\n",
    "                \n",
    "                # Chuẩn bị văn bản để dịch\n",
    "                texts_to_translate = [item[\"text\"] for item in text_group]\n",
    "                \n",
    "                # Dịch cả batch\n",
    "                translated_batch = translator(texts_to_translate)\n",
    "                \n",
    "                # Xử lý kết quả\n",
    "                for i, (item, translated) in enumerate(zip(text_group, translated_batch)):\n",
    "                    translated_text = translated[\"translation_text\"]\n",
    "                    \n",
    "                    if not translated_text or translated_text.strip() == \"\":\n",
    "                        # Fallback to Google Translate\n",
    "                        translated_text = self.__google_translate(item[\"text\"])\n",
    "                        translation_type = \"deep\"\n",
    "                    else:\n",
    "                        translation_type = \"nlln\"\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"audio_path\": item[\"path\"],\n",
    "                        \"translation_type\": translation_type,\n",
    "                        \"origin\": item[\"text\"],\n",
    "                        \"translation\": translated_text\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi khi dịch batch ngôn ngữ {src_lang_code}: {str(e)}\")\n",
    "                # Dùng Google Translate cho từng văn bản\n",
    "                for item in text_group:\n",
    "                    translated = self.__google_translate(item[\"text\"])\n",
    "                    results.append({\n",
    "                        \"audio_path\": item[\"path\"],\n",
    "                        \"translation_type\": \"deep\",\n",
    "                        \"origin\": item[\"text\"],\n",
    "                        \"translation\": translated\n",
    "                    })\n",
    "        \n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "processor = Audio2Text(max_text_gen = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 132/26544 [04:38<19:03:23,  2.60s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "  1%|          | 302/26544 [09:38<16:39:28,  2.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "  1%|          | 304/26544 [09:48<24:06:30,  3.31s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "  4%|▍         | 1121/26544 [34:04<14:27:42,  2.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "  5%|▍         | 1268/26544 [39:45<21:22:28,  3.04s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "  5%|▍         | 1312/26544 [42:02<21:42:28,  3.10s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "  8%|▊         | 2069/26544 [1:11:07<9:51:54,  1.45s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "  8%|▊         | 2144/26544 [1:13:12<10:35:42,  1.56s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 13%|█▎        | 3356/26544 [1:47:41<10:49:36,  1.68s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 13%|█▎        | 3357/26544 [1:47:47<19:39:01,  3.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 13%|█▎        | 3363/26544 [1:48:03<13:54:02,  2.16s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 13%|█▎        | 3374/26544 [1:48:30<12:48:49,  1.99s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 13%|█▎        | 3398/26544 [1:49:21<15:10:39,  2.36s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 14%|█▍        | 3747/26544 [2:03:11<20:50:01,  3.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 14%|█▍        | 3757/26544 [2:03:47<18:51:01,  2.98s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 14%|█▍        | 3798/26544 [2:05:56<16:42:07,  2.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 14%|█▍        | 3806/26544 [2:06:23<18:35:56,  2.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 15%|█▍        | 3961/26544 [2:14:20<20:53:55,  3.33s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▌        | 4311/26544 [2:26:23<17:11:50,  2.78s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4322/26544 [2:27:00<19:29:26,  3.16s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4326/26544 [2:27:16<21:27:00,  3.48s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4331/26544 [2:27:31<17:06:28,  2.77s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4342/26544 [2:28:05<14:18:41,  2.32s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4343/26544 [2:28:11<20:19:12,  3.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4345/26544 [2:28:20<23:10:05,  3.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4346/26544 [2:28:27<29:30:52,  4.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4349/26544 [2:28:39<23:50:58,  3.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4350/26544 [2:28:47<30:02:53,  4.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4351/26544 [2:28:54<35:31:52,  5.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4353/26544 [2:29:04<31:01:39,  5.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4354/26544 [2:29:10<33:26:20,  5.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4355/26544 [2:29:18<36:46:24,  5.97s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4357/26544 [2:29:28<32:59:59,  5.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4358/26544 [2:29:35<35:54:51,  5.83s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4366/26544 [2:30:05<20:39:27,  3.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4369/26544 [2:30:17<21:22:10,  3.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4376/26544 [2:30:40<17:38:47,  2.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 16%|█▋        | 4377/26544 [2:30:46<24:00:53,  3.90s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 17%|█▋        | 4382/26544 [2:31:00<16:05:55,  2.62s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4763/26544 [2:45:31<11:37:21,  1.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4767/26544 [2:45:44<14:37:10,  2.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4775/26544 [2:46:06<13:46:21,  2.28s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4776/26544 [2:46:12<20:59:33,  3.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4778/26544 [2:46:21<22:58:38,  3.80s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4780/26544 [2:46:29<21:33:47,  3.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4783/26544 [2:46:40<20:29:24,  3.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4788/26544 [2:46:52<12:43:21,  2.11s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4800/26544 [2:47:18<9:54:56,  1.64s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4803/26544 [2:47:27<13:57:10,  2.31s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4805/26544 [2:47:35<18:41:19,  3.09s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4818/26544 [2:48:04<13:19:13,  2.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4822/26544 [2:48:17<15:35:21,  2.58s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4825/26544 [2:48:27<16:18:30,  2.70s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4827/26544 [2:48:36<20:19:54,  3.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4835/26544 [2:48:56<12:12:51,  2.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4836/26544 [2:49:02<19:37:47,  3.26s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4837/26544 [2:49:08<24:31:08,  4.07s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4839/26544 [2:49:16<23:12:51,  3.85s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4840/26544 [2:49:21<26:49:41,  4.45s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4841/26544 [2:49:27<29:03:13,  4.82s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4842/26544 [2:49:32<29:56:03,  4.97s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4843/26544 [2:49:38<31:41:19,  5.26s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4844/26544 [2:49:46<35:19:28,  5.86s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4848/26544 [2:49:56<18:02:36,  2.99s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 18%|█▊        | 4849/26544 [2:50:03<25:21:00,  4.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 22%|██▏       | 5772/26544 [3:23:26<10:58:01,  1.90s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 22%|██▏       | 5939/26544 [3:28:56<17:15:34,  3.02s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 23%|██▎       | 6035/26544 [3:34:05<10:54:56,  1.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 23%|██▎       | 6104/26544 [3:36:26<10:17:32,  1.81s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 23%|██▎       | 6107/26544 [3:36:39<16:37:45,  2.93s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 23%|██▎       | 6114/26544 [3:37:00<13:32:14,  2.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6391/26544 [3:47:48<19:59:02,  3.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6392/26544 [3:47:58<31:55:37,  5.70s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6393/26544 [3:48:06<35:58:07,  6.43s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6394/26544 [3:48:17<42:34:40,  7.61s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6395/26544 [3:48:24<41:28:42,  7.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6396/26544 [3:48:31<41:05:37,  7.34s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6397/26544 [3:48:40<44:15:41,  7.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6398/26544 [3:48:48<44:20:09,  7.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6399/26544 [3:48:57<46:15:45,  8.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6400/26544 [3:49:08<50:43:38,  9.07s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6401/26544 [3:49:15<47:25:29,  8.48s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6402/26544 [3:49:22<45:08:09,  8.07s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6403/26544 [3:49:29<42:58:39,  7.68s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6404/26544 [3:49:37<43:17:31,  7.74s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6405/26544 [3:49:44<41:19:20,  7.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6406/26544 [3:49:52<42:44:43,  7.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6407/26544 [3:49:59<41:45:09,  7.46s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6408/26544 [3:50:06<41:14:23,  7.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6409/26544 [3:50:13<41:11:53,  7.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6410/26544 [3:50:22<42:40:52,  7.63s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6411/26544 [3:50:29<41:59:07,  7.51s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6412/26544 [3:50:35<40:18:51,  7.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6413/26544 [3:50:43<40:34:43,  7.26s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6414/26544 [3:50:51<42:36:25,  7.62s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6415/26544 [3:50:59<42:26:51,  7.59s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6416/26544 [3:51:06<41:32:51,  7.43s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6417/26544 [3:51:13<41:12:49,  7.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6418/26544 [3:51:20<41:01:01,  7.34s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6419/26544 [3:51:27<40:00:48,  7.16s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6420/26544 [3:51:34<39:23:48,  7.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6421/26544 [3:51:41<39:26:08,  7.06s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6422/26544 [3:51:48<40:26:35,  7.24s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6423/26544 [3:51:59<45:53:10,  8.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6424/26544 [3:52:06<43:36:12,  7.80s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6425/26544 [3:52:12<41:36:48,  7.45s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6426/26544 [3:52:19<40:44:05,  7.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6427/26544 [3:52:26<39:46:03,  7.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6428/26544 [3:52:32<38:28:33,  6.89s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6429/26544 [3:52:39<38:14:48,  6.85s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6430/26544 [3:52:46<38:47:29,  6.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6431/26544 [3:52:53<39:08:26,  7.01s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6432/26544 [3:53:01<40:05:36,  7.18s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6433/26544 [3:53:08<40:12:30,  7.20s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6434/26544 [3:53:16<40:36:32,  7.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 24%|██▍       | 6435/26544 [3:53:23<41:14:57,  7.38s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 25%|██▍       | 6504/26544 [3:55:56<12:52:10,  2.31s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 25%|██▍       | 6607/26544 [3:58:47<3:21:07,  1.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2015-10-04 - 21-30 Atl. Madrid 1 - 1 Real Madrid/chunk_70/chunk_70.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2015-10-04 - 21-30 Atl. Madrid 1 - 1 Real Madrid/chunk_70/chunk_70.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6903/26544 [4:06:51<15:54:01,  2.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6910/26544 [4:07:17<16:39:32,  3.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6911/26544 [4:07:24<24:00:49,  4.40s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6914/26544 [4:07:38<22:29:17,  4.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6916/26544 [4:07:49<25:33:26,  4.69s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6924/26544 [4:08:16<17:07:55,  3.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6928/26544 [4:08:35<23:01:08,  4.22s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6936/26544 [4:09:08<19:45:24,  3.63s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6938/26544 [4:09:17<21:55:26,  4.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6940/26544 [4:09:28<24:24:45,  4.48s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6946/26544 [4:09:56<26:46:04,  4.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6953/26544 [4:10:21<17:53:37,  3.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6954/26544 [4:10:28<23:57:26,  4.40s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6961/26544 [4:10:55<17:50:10,  3.28s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▌       | 6963/26544 [4:11:07<24:37:46,  4.53s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▋       | 6969/26544 [4:11:31<19:57:37,  3.67s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▋       | 6975/26544 [4:11:57<21:53:18,  4.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 26%|██▋       | 6977/26544 [4:12:08<25:13:52,  4.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 7985/26544 [4:51:36<12:26:33,  2.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 7988/26544 [4:51:52<20:43:13,  4.02s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 7989/26544 [4:52:02<29:53:54,  5.80s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 7995/26544 [4:52:30<22:09:37,  4.30s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 8003/26544 [4:53:04<21:51:01,  4.24s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 8004/26544 [4:53:10<25:06:15,  4.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 8011/26544 [4:53:36<17:52:34,  3.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 8018/26544 [4:54:03<18:17:32,  3.55s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 8020/26544 [4:54:13<20:50:50,  4.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 8050/26544 [4:56:05<17:33:42,  3.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 30%|███       | 8064/26544 [4:56:54<15:24:13,  3.00s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8164/26544 [5:00:21<9:33:55,  1.87s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8166/26544 [5:00:30<15:56:35,  3.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8169/26544 [5:00:42<17:08:16,  3.36s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8170/26544 [5:00:49<22:12:48,  4.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8171/26544 [5:00:57<27:51:58,  5.46s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8172/26544 [5:01:03<29:12:45,  5.72s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8173/26544 [5:01:09<30:05:15,  5.90s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8174/26544 [5:01:16<30:46:50,  6.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8175/26544 [5:01:24<33:51:44,  6.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8176/26544 [5:01:30<33:31:41,  6.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8177/26544 [5:01:38<35:04:26,  6.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8178/26544 [5:01:45<34:51:35,  6.83s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8179/26544 [5:01:51<34:28:13,  6.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8180/26544 [5:01:58<35:24:04,  6.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8181/26544 [5:02:06<36:43:21,  7.20s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8182/26544 [5:02:12<35:11:06,  6.90s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8183/26544 [5:02:19<34:56:34,  6.85s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8184/26544 [5:02:26<34:56:08,  6.85s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8185/26544 [5:02:32<33:41:57,  6.61s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8186/26544 [5:02:39<34:04:33,  6.68s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8187/26544 [5:02:46<33:57:11,  6.66s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8188/26544 [5:02:53<35:25:41,  6.95s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8189/26544 [5:02:59<33:23:30,  6.55s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8190/26544 [5:03:05<32:48:05,  6.43s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8191/26544 [5:03:15<37:56:52,  7.44s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8193/26544 [5:03:24<29:42:24,  5.83s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8194/26544 [5:03:31<31:14:30,  6.13s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8196/26544 [5:03:40<25:53:20,  5.08s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8197/26544 [5:03:47<30:09:52,  5.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8198/26544 [5:03:54<30:46:32,  6.04s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8199/26544 [5:04:01<32:52:29,  6.45s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8200/26544 [5:04:08<33:15:09,  6.53s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8201/26544 [5:04:15<34:03:15,  6.68s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8202/26544 [5:04:21<33:23:45,  6.55s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8203/26544 [5:04:30<36:23:04,  7.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8204/26544 [5:04:36<35:01:51,  6.88s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8205/26544 [5:04:42<34:11:38,  6.71s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8206/26544 [5:04:49<34:02:12,  6.68s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8207/26544 [5:04:55<32:35:37,  6.40s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8208/26544 [5:05:01<32:18:26,  6.34s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8210/26544 [5:05:10<26:38:49,  5.23s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8211/26544 [5:05:16<28:05:51,  5.52s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8212/26544 [5:05:22<29:23:25,  5.77s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8213/26544 [5:05:29<30:25:36,  5.98s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8214/26544 [5:05:35<31:01:57,  6.09s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8215/26544 [5:05:43<34:14:36,  6.73s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8217/26544 [5:05:53<28:17:30,  5.56s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8219/26544 [5:06:02<24:51:50,  4.88s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8220/26544 [5:06:09<27:19:23,  5.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8222/26544 [5:06:18<23:44:18,  4.66s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8225/26544 [5:06:29<18:46:49,  3.69s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8228/26544 [5:06:39<16:07:59,  3.17s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8237/26544 [5:07:06<14:51:14,  2.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8239/26544 [5:07:15<17:21:02,  3.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8240/26544 [5:07:21<21:36:43,  4.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8242/26544 [5:07:29<20:20:09,  4.00s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8244/26544 [5:07:37<19:36:39,  3.86s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8246/26544 [5:07:47<21:08:19,  4.16s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8249/26544 [5:08:02<21:29:09,  4.23s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8253/26544 [5:08:16<16:51:11,  3.32s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 31%|███       | 8254/26544 [5:08:22<20:57:23,  4.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8448/26544 [5:17:04<7:52:49,  1.57s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8480/26544 [5:18:05<9:50:41,  1.96s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8485/26544 [5:18:18<11:01:54,  2.20s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8495/26544 [5:18:38<8:42:08,  1.74s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8501/26544 [5:18:51<9:07:05,  1.82s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8502/26544 [5:18:57<15:32:29,  3.10s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8511/26544 [5:19:18<10:33:37,  2.11s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 32%|███▏      | 8516/26544 [5:19:29<8:29:08,  1.69s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 34%|███▍      | 9034/26544 [5:32:14<8:31:02,  1.75s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 35%|███▍      | 9266/26544 [5:38:57<2:37:43,  1.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2015-12-09 - 22-45 D. Zagreb 0 - 2 Bayern Munich/chunk_62/chunk_62.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2015-12-09 - 22-45 D. Zagreb 0 - 2 Bayern Munich/chunk_62/chunk_62.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 9280/26544 [5:39:02<1:51:01,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2015-12-09 - 22-45 D. Zagreb 0 - 2 Bayern Munich/chunk_79/chunk_79.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2015-12-09 - 22-45 D. Zagreb 0 - 2 Bayern Munich/chunk_79/chunk_79.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 9286/26544 [5:39:05<2:50:56,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2015-12-12 - 17-30 Bayern Munich 2 - 0 Ingolstadt/chunk_2/chunk_2.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2015-12-12 - 17-30 Bayern Munich 2 - 0 Ingolstadt/chunk_2/chunk_2.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 9397/26544 [5:41:56<5:52:28,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2015-12-26 - 18-00 Manchester City 4 - 1 Sunderland/chunk_48/chunk_48.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2015-12-26 - 18-00 Manchester City 4 - 1 Sunderland/chunk_48/chunk_48.mp3'\n",
      "Lỗi khi xử lý F:/video_classification/2015-12-26 - 18-00 Manchester City 4 - 1 Sunderland/chunk_49/chunk_49.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2015-12-26 - 18-00 Manchester City 4 - 1 Sunderland/chunk_49/chunk_49.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 9631/26544 [5:47:26<2:01:48,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2016-01-24 - 22-30 Betis 1 - 1 Real Madrid/chunk_89/chunk_89.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2016-01-24 - 22-30 Betis 1 - 1 Real Madrid/chunk_89/chunk_89.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 9987/26544 [5:55:23<7:37:50,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2016-02-21 - 18-00 Malaga 1 - 1 Real Madrid/chunk_0/chunk_0.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2016-02-21 - 18-00 Malaga 1 - 1 Real Madrid/chunk_0/chunk_0.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 10296/26544 [6:02:06<1:53:05,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lỗi khi xử lý F:/video_classification/2016-03-13 - 22-30 Las Palmas 1 - 2 Real Madrid/chunk_1/chunk_1.mp3: [Errno 1094995529] Invalid data found when processing input: 'F:/video_classification/2016-03-13 - 22-30 Las Palmas 1 - 2 Real Madrid/chunk_1/chunk_1.mp3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10638/26544 [6:13:37<6:59:08,  1.58s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 40%|████      | 10653/26544 [6:14:07<7:53:26,  1.79s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 40%|████      | 10655/26544 [6:14:15<11:33:17,  2.62s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 40%|████      | 10662/26544 [6:14:32<8:36:19,  1.95s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 40%|████      | 10685/26544 [6:15:15<7:34:46,  1.72s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 40%|████      | 10708/26544 [6:15:55<6:14:32,  1.42s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████      | 10893/26544 [6:23:41<11:03:36,  2.54s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10979/26544 [6:27:21<13:29:34,  3.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10980/26544 [6:27:28<18:39:45,  4.32s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10982/26544 [6:27:38<19:48:07,  4.58s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10985/26544 [6:27:51<18:02:01,  4.17s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10987/26544 [6:28:02<19:44:11,  4.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10992/26544 [6:28:24<15:52:18,  3.67s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10995/26544 [6:28:37<16:15:13,  3.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 10997/26544 [6:28:49<20:47:00,  4.81s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 41%|████▏     | 11007/26544 [6:29:29<14:59:56,  3.48s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 42%|████▏     | 11025/26544 [6:30:26<11:53:44,  2.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 42%|████▏     | 11031/26544 [6:30:52<14:26:34,  3.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 42%|████▏     | 11034/26544 [6:31:06<16:58:56,  3.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 42%|████▏     | 11162/26544 [6:35:20<13:22:41,  3.13s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 43%|████▎     | 11294/26544 [6:42:31<10:55:25,  2.58s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 43%|████▎     | 11482/26544 [6:49:21<8:58:17,  2.14s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 44%|████▍     | 11719/26544 [6:56:59<13:46:38,  3.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 44%|████▍     | 11730/26544 [6:57:39<13:38:23,  3.31s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 44%|████▍     | 11737/26544 [6:58:04<13:02:47,  3.17s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 44%|████▍     | 11774/26544 [6:59:59<11:43:13,  2.86s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 11996/26544 [7:07:21<11:20:16,  2.81s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12006/26544 [7:07:56<12:16:33,  3.04s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12024/26544 [7:08:49<11:11:30,  2.77s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12028/26544 [7:09:05<13:17:43,  3.30s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12029/26544 [7:09:13<18:36:25,  4.61s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12031/26544 [7:09:22<17:48:00,  4.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12034/26544 [7:09:34<15:52:24,  3.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12038/26544 [7:09:49<14:00:54,  3.48s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12039/26544 [7:09:56<17:28:00,  4.34s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 45%|████▌     | 12047/26544 [7:10:22<12:11:05,  3.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 47%|████▋     | 12416/26544 [7:29:13<10:04:49,  2.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 49%|████▉     | 13047/26544 [7:51:03<12:10:02,  3.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 49%|████▉     | 13055/26544 [7:51:30<10:20:49,  2.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 49%|████▉     | 13066/26544 [7:52:03<9:53:57,  2.64s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 50%|████▉     | 13165/26544 [7:57:07<10:49:58,  2.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13437/26544 [8:08:55<9:31:57,  2.62s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13438/26544 [8:09:05<17:12:42,  4.73s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13440/26544 [8:09:18<20:17:22,  5.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13441/26544 [8:09:28<24:27:37,  6.72s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13443/26544 [8:09:39<21:37:15,  5.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13452/26544 [8:10:13<11:48:13,  3.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13456/26544 [8:10:31<13:40:39,  3.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13458/26544 [8:10:44<19:05:48,  5.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13463/26544 [8:11:08<15:02:51,  4.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13471/26544 [8:11:44<14:25:19,  3.97s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13474/26544 [8:11:59<15:48:42,  4.36s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13476/26544 [8:12:12<18:13:52,  5.02s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13479/26544 [8:12:25<15:16:21,  4.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13483/26544 [8:12:42<13:43:06,  3.78s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13503/26544 [8:13:53<12:24:25,  3.43s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 51%|█████     | 13520/26544 [8:15:00<14:28:20,  4.00s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 52%|█████▏    | 13839/26544 [8:26:15<7:50:25,  2.22s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13979/26544 [8:31:24<9:51:06,  2.82s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13980/26544 [8:31:31<14:16:35,  4.09s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13983/26544 [8:31:45<14:17:35,  4.10s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13985/26544 [8:31:56<17:08:42,  4.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13986/26544 [8:32:03<18:54:45,  5.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13988/26544 [8:32:14<17:57:08,  5.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13991/26544 [8:32:26<14:49:06,  4.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13996/26544 [8:32:43<11:09:18,  3.20s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 13999/26544 [8:32:59<15:31:56,  4.46s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14003/26544 [8:33:17<13:53:31,  3.99s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14007/26544 [8:33:40<16:48:15,  4.83s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14008/26544 [8:33:46<18:38:43,  5.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14027/26544 [8:34:55<11:58:05,  3.44s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14032/26544 [8:35:15<12:28:57,  3.59s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14040/26544 [8:35:42<10:01:33,  2.89s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14043/26544 [8:35:58<14:33:43,  4.19s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14048/26544 [8:36:18<13:53:13,  4.00s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14049/26544 [8:36:25<16:43:54,  4.82s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14050/26544 [8:36:34<21:09:28,  6.10s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14054/26544 [8:36:54<17:16:42,  4.98s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14061/26544 [8:37:23<14:31:11,  4.19s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14062/26544 [8:37:30<17:37:04,  5.08s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14063/26544 [8:37:38<20:12:03,  5.83s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14065/26544 [8:37:48<18:02:12,  5.20s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 53%|█████▎    | 14067/26544 [8:37:57<16:29:56,  4.76s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 55%|█████▌    | 14710/26544 [8:59:05<9:24:47,  2.86s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 55%|█████▌    | 14719/26544 [8:59:38<10:37:46,  3.24s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 55%|█████▌    | 14721/26544 [8:59:50<14:16:40,  4.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 55%|█████▌    | 14722/26544 [8:59:58<17:46:10,  5.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 55%|█████▌    | 14725/26544 [9:00:14<16:54:37,  5.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 55%|█████▌    | 14728/26544 [9:00:28<14:25:11,  4.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 55%|█████▌    | 14730/26544 [9:00:39<15:18:21,  4.66s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14732/26544 [9:00:50<16:06:19,  4.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14736/26544 [9:01:07<12:42:46,  3.88s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14750/26544 [9:01:53<9:21:03,  2.85s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14755/26544 [9:02:12<10:12:40,  3.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14769/26544 [9:02:52<7:47:07,  2.38s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14772/26544 [9:03:05<11:21:19,  3.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14776/26544 [9:03:21<12:21:20,  3.78s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 56%|█████▌    | 14782/26544 [9:03:49<13:18:42,  4.07s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15612/26544 [9:36:25<6:12:38,  2.05s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15613/26544 [9:36:34<12:23:10,  4.08s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15615/26544 [9:36:46<14:31:12,  4.78s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15616/26544 [9:36:53<17:06:43,  5.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15620/26544 [9:37:10<12:23:02,  4.08s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15621/26544 [9:37:18<15:50:28,  5.22s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15623/26544 [9:37:30<16:52:06,  5.56s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15628/26544 [9:37:51<11:40:13,  3.85s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15642/26544 [9:38:40<9:00:05,  2.97s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15644/26544 [9:38:51<12:10:59,  4.02s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15653/26544 [9:39:22<7:56:12,  2.62s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15661/26544 [9:39:49<8:30:08,  2.81s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15664/26544 [9:40:04<12:37:41,  4.18s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15665/26544 [9:40:12<16:02:55,  5.31s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15667/26544 [9:40:22<14:38:23,  4.85s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15670/26544 [9:40:34<12:48:53,  4.24s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15691/26544 [9:41:42<9:05:03,  3.01s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15692/26544 [9:41:49<12:13:05,  4.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 59%|█████▉    | 15722/26544 [9:42:49<6:21:40,  2.12s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 60%|█████▉    | 15813/26544 [9:45:50<8:25:17,  2.83s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 60%|█████▉    | 15822/26544 [9:46:25<11:10:01,  3.75s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 60%|█████▉    | 15833/26544 [9:47:14<10:07:43,  3.40s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16062/26544 [9:56:22<7:57:44,  2.73s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16063/26544 [9:56:29<11:51:16,  4.07s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16064/26544 [9:56:37<15:13:59,  5.23s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16065/26544 [9:56:44<16:33:32,  5.69s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16066/26544 [9:56:51<17:46:21,  6.11s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16067/26544 [9:56:57<18:05:25,  6.22s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16068/26544 [9:57:04<18:38:52,  6.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16074/26544 [9:57:25<9:57:25,  3.42s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16081/26544 [9:57:50<9:08:48,  3.15s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16090/26544 [9:58:25<9:32:40,  3.29s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16100/26544 [9:58:59<9:31:07,  3.28s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16106/26544 [9:59:24<10:20:27,  3.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16108/26544 [9:59:32<11:02:47,  3.81s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16113/26544 [9:59:53<10:33:34,  3.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16119/26544 [10:00:15<9:20:24,  3.23s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16120/26544 [10:00:22<12:43:42,  4.40s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16130/26544 [10:01:00<10:02:18,  3.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16132/26544 [10:01:11<12:23:20,  4.28s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 61%|██████    | 16134/26544 [10:01:20<12:21:32,  4.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16698/26544 [10:20:47<4:25:06,  1.62s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16699/26544 [10:20:55<9:35:09,  3.51s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16702/26544 [10:21:11<12:05:11,  4.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16720/26544 [10:22:11<8:49:10,  3.23s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16725/26544 [10:22:35<12:11:13,  4.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16730/26544 [10:22:56<10:01:55,  3.68s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16742/26544 [10:23:39<8:16:06,  3.04s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16764/26544 [10:24:55<8:09:22,  3.00s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 63%|██████▎   | 16802/26544 [10:26:53<6:12:21,  2.29s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 64%|██████▍   | 17090/26544 [10:40:28<1:54:43,  1.37it/s] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 65%|██████▍   | 17129/26544 [10:41:06<2:35:27,  1.01it/s]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17683/26544 [11:01:41<8:15:03,  3.35s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17719/26544 [11:03:35<8:07:25,  3.31s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17722/26544 [11:03:50<9:32:01,  3.89s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17724/26544 [11:04:02<11:57:34,  4.88s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17726/26544 [11:04:11<11:18:40,  4.62s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17727/26544 [11:04:19<13:24:11,  5.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17730/26544 [11:04:34<12:00:30,  4.90s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17731/26544 [11:04:43<15:02:27,  6.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17741/26544 [11:05:19<7:25:28,  3.04s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17746/26544 [11:05:38<8:00:21,  3.28s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17748/26544 [11:05:51<11:17:48,  4.62s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17751/26544 [11:06:06<10:37:49,  4.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17752/26544 [11:06:12<12:14:27,  5.01s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17753/26544 [11:06:19<13:22:11,  5.48s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17755/26544 [11:06:30<13:17:46,  5.45s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17759/26544 [11:06:49<10:42:47,  4.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17760/26544 [11:06:57<13:21:23,  5.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17770/26544 [11:07:30<7:27:35,  3.06s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17787/26544 [11:08:28<6:47:41,  2.79s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17788/26544 [11:08:35<9:45:57,  4.02s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17799/26544 [11:09:17<8:34:15,  3.53s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17800/26544 [11:09:25<11:24:13,  4.70s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 67%|██████▋   | 17801/26544 [11:09:32<13:04:18,  5.38s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18001/26544 [11:16:07<3:54:53,  1.65s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18004/26544 [11:16:16<5:21:36,  2.26s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18006/26544 [11:16:23<6:40:05,  2.81s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18014/26544 [11:16:41<4:52:37,  2.06s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18022/26544 [11:16:57<4:06:19,  1.73s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18026/26544 [11:17:09<5:03:39,  2.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18027/26544 [11:17:15<7:58:42,  3.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18042/26544 [11:17:45<3:45:57,  1.59s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18051/26544 [11:18:05<4:13:14,  1.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 68%|██████▊   | 18066/26544 [11:18:36<3:53:03,  1.65s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19641/26544 [12:15:04<5:20:45,  2.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19655/26544 [12:15:55<6:02:06,  3.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19659/26544 [12:16:13<7:15:09,  3.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19667/26544 [12:16:44<6:07:54,  3.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19671/26544 [12:17:04<7:40:18,  4.02s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19675/26544 [12:17:22<7:42:29,  4.04s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19687/26544 [12:18:08<7:23:33,  3.88s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19698/26544 [12:18:48<5:45:38,  3.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19699/26544 [12:18:56<8:21:44,  4.40s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19700/26544 [12:19:02<9:32:28,  5.02s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19707/26544 [12:19:34<8:28:21,  4.46s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19714/26544 [12:20:00<6:21:49,  3.35s/it] `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 74%|███████▍  | 19730/26544 [12:20:58<6:09:05,  3.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 77%|███████▋  | 20389/26544 [12:49:55<6:30:46,  3.81s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 77%|███████▋  | 20416/26544 [12:51:26<5:22:12,  3.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 77%|███████▋  | 20464/26544 [12:54:01<5:09:20,  3.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 77%|███████▋  | 20507/26544 [12:56:28<4:48:28,  2.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 77%|███████▋  | 20521/26544 [12:57:15<4:47:04,  2.86s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 80%|████████  | 21259/26544 [13:24:48<3:14:16,  2.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 81%|████████  | 21558/26544 [13:37:34<4:24:22,  3.18s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 81%|████████▏ | 21591/26544 [13:39:38<4:18:10,  3.13s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 81%|████████▏ | 21628/26544 [13:41:51<5:42:57,  4.19s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 82%|████████▏ | 21778/26544 [13:48:26<3:27:20,  2.61s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 82%|████████▏ | 21793/26544 [13:49:16<4:24:40,  3.34s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 82%|████████▏ | 21800/26544 [13:49:39<4:08:48,  3.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 82%|████████▏ | 21811/26544 [13:50:14<3:50:36,  2.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 82%|████████▏ | 21817/26544 [13:50:35<3:51:39,  2.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 83%|████████▎ | 22114/26544 [14:02:45<4:34:56,  3.72s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 83%|████████▎ | 22126/26544 [14:03:24<3:50:36,  3.13s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22371/26544 [14:16:59<3:48:40,  3.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22372/26544 [14:17:09<6:03:03,  5.22s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22381/26544 [14:17:43<3:54:04,  3.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22382/26544 [14:17:52<6:10:03,  5.33s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22389/26544 [14:18:26<4:51:56,  4.22s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22392/26544 [14:18:42<5:11:12,  4.50s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22394/26544 [14:18:55<6:04:11,  5.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22405/26544 [14:19:47<5:11:19,  4.51s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22406/26544 [14:19:53<5:42:33,  4.97s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22409/26544 [14:20:05<4:48:30,  4.19s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 84%|████████▍ | 22415/26544 [14:20:27<3:51:27,  3.36s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 85%|████████▍ | 22432/26544 [14:21:23<4:01:52,  3.53s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22721/26544 [14:35:37<2:14:34,  2.11s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22725/26544 [14:35:54<3:40:17,  3.46s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22726/26544 [14:36:01<4:47:29,  4.52s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22738/26544 [14:36:56<5:03:21,  4.78s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22739/26544 [14:37:02<5:33:37,  5.26s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22749/26544 [14:37:39<3:36:34,  3.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22750/26544 [14:37:49<5:34:26,  5.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22783/26544 [14:39:34<3:02:31,  2.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22786/26544 [14:39:46<3:37:35,  3.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22787/26544 [14:39:53<4:37:30,  4.43s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22788/26544 [14:39:59<5:15:39,  5.04s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22789/26544 [14:40:06<5:44:11,  5.50s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 86%|████████▌ | 22807/26544 [14:41:08<3:17:50,  3.18s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 87%|████████▋ | 23005/26544 [14:48:21<2:52:29,  2.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 87%|████████▋ | 23015/26544 [14:48:54<2:44:03,  2.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 87%|████████▋ | 23017/26544 [14:49:05<3:57:50,  4.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 87%|████████▋ | 23042/26544 [14:50:26<3:04:00,  3.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 88%|████████▊ | 23473/26544 [15:09:01<1:21:41,  1.60s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24170/26544 [15:36:53<48:43,  1.23s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24171/26544 [15:37:00<2:02:11,  3.09s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24178/26544 [15:37:26<2:04:09,  3.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24179/26544 [15:37:34<2:54:35,  4.43s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24185/26544 [15:37:59<2:22:27,  3.62s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24189/26544 [15:38:16<2:33:42,  3.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24190/26544 [15:38:24<3:21:18,  5.13s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24209/26544 [15:39:28<1:57:53,  3.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24214/26544 [15:39:48<2:21:37,  3.65s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24217/26544 [15:40:03<2:52:17,  4.44s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24218/26544 [15:40:10<3:24:21,  5.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████ | 24221/26544 [15:40:23<2:48:18,  4.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████▏| 24224/26544 [15:40:35<2:31:43,  3.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████▏| 24229/26544 [15:40:57<2:23:13,  3.71s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████▏| 24238/26544 [15:41:27<1:52:24,  2.92s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████▏| 24239/26544 [15:41:33<2:32:21,  3.97s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 91%|█████████▏| 24254/26544 [15:42:22<1:52:38,  2.95s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24353/26544 [15:47:21<1:46:11,  2.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24354/26544 [15:47:28<2:31:11,  4.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24355/26544 [15:47:35<3:06:32,  5.11s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24359/26544 [15:47:52<2:23:23,  3.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24361/26544 [15:48:02<2:35:40,  4.28s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24362/26544 [15:48:09<3:11:11,  5.26s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24363/26544 [15:48:18<3:43:10,  6.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24364/26544 [15:48:24<3:52:01,  6.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24365/26544 [15:48:32<4:01:30,  6.65s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24366/26544 [15:48:41<4:25:32,  7.32s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24367/26544 [15:48:47<4:19:48,  7.16s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24368/26544 [15:48:56<4:34:26,  7.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24369/26544 [15:49:03<4:32:08,  7.51s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24370/26544 [15:49:12<4:46:37,  7.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24371/26544 [15:49:19<4:34:27,  7.58s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24372/26544 [15:49:26<4:26:05,  7.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24373/26544 [15:49:32<4:18:19,  7.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24375/26544 [15:49:44<3:41:52,  6.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24376/26544 [15:49:51<3:51:49,  6.42s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24378/26544 [15:50:00<3:13:51,  5.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24379/26544 [15:50:07<3:34:05,  5.93s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24381/26544 [15:50:17<3:05:54,  5.16s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24382/26544 [15:50:24<3:21:25,  5.59s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24383/26544 [15:50:32<3:50:05,  6.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24384/26544 [15:50:39<3:58:00,  6.61s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24385/26544 [15:50:47<4:07:32,  6.88s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24386/26544 [15:50:54<4:13:38,  7.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24389/26544 [15:51:06<2:52:23,  4.80s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24390/26544 [15:51:13<3:14:13,  5.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24391/26544 [15:51:23<4:00:58,  6.72s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24393/26544 [15:51:32<3:17:02,  5.50s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24394/26544 [15:51:39<3:29:20,  5.84s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24395/26544 [15:51:46<3:37:13,  6.06s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24396/26544 [15:51:52<3:43:13,  6.24s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24397/26544 [15:51:59<3:45:04,  6.29s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24398/26544 [15:52:07<4:06:42,  6.90s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24399/26544 [15:52:15<4:23:25,  7.37s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24400/26544 [15:52:23<4:27:02,  7.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24401/26544 [15:52:30<4:19:43,  7.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24402/26544 [15:52:38<4:24:21,  7.40s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24403/26544 [15:52:46<4:36:27,  7.75s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24404/26544 [15:52:54<4:32:26,  7.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24405/26544 [15:53:01<4:28:12,  7.52s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24406/26544 [15:53:08<4:25:05,  7.44s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24407/26544 [15:53:15<4:18:55,  7.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24408/26544 [15:53:23<4:28:01,  7.53s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24409/26544 [15:53:31<4:28:13,  7.54s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24410/26544 [15:53:38<4:22:53,  7.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24411/26544 [15:53:45<4:18:25,  7.27s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24412/26544 [15:53:52<4:16:19,  7.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24413/26544 [15:53:58<4:10:15,  7.05s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24415/26544 [15:54:07<3:18:14,  5.59s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24416/26544 [15:54:14<3:31:58,  5.98s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24417/26544 [15:54:21<3:36:50,  6.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24418/26544 [15:54:27<3:39:54,  6.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24419/26544 [15:54:34<3:43:23,  6.31s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24420/26544 [15:54:40<3:42:12,  6.28s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24421/26544 [15:54:47<3:54:11,  6.62s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24422/26544 [15:54:54<3:56:33,  6.69s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24425/26544 [15:55:07<2:47:39,  4.75s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24427/26544 [15:55:16<2:33:27,  4.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24428/26544 [15:55:24<3:13:04,  5.47s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24429/26544 [15:55:32<3:36:36,  6.14s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24431/26544 [15:55:41<2:59:29,  5.10s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24434/26544 [15:55:53<2:22:43,  4.06s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24435/26544 [15:56:00<2:53:35,  4.94s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24436/26544 [15:56:07<3:17:45,  5.63s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24438/26544 [15:56:17<2:55:58,  5.01s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 92%|█████████▏| 24439/26544 [15:56:24<3:15:13,  5.56s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25176/26544 [16:29:27<56:56,  2.50s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25177/26544 [16:29:34<1:28:36,  3.89s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25179/26544 [16:29:46<1:47:09,  4.71s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25180/26544 [16:29:54<2:07:34,  5.61s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25182/26544 [16:30:07<2:09:25,  5.70s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25186/26544 [16:30:23<1:33:54,  4.15s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25190/26544 [16:30:40<1:26:44,  3.84s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25191/26544 [16:30:47<1:51:43,  4.95s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25192/26544 [16:30:55<2:11:32,  5.84s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25193/26544 [16:31:04<2:29:55,  6.66s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25202/26544 [16:31:34<1:02:37,  2.80s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25207/26544 [16:31:52<1:06:56,  3.00s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25209/26544 [16:32:02<1:24:13,  3.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▍| 25212/26544 [16:32:14<1:19:57,  3.60s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▌| 25221/26544 [16:32:46<1:06:12,  3.00s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 95%|█████████▌| 25226/26544 [16:33:05<1:10:24,  3.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 96%|█████████▌| 25545/26544 [16:46:22<25:52,  1.55s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 96%|█████████▋| 25595/26544 [16:47:51<27:04,  1.71s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 96%|█████████▋| 25602/26544 [16:48:07<27:29,  1.75s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 96%|█████████▋| 25603/26544 [16:48:13<48:05,  3.07s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 96%|█████████▋| 25607/26544 [16:48:24<39:18,  2.52s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25615/26544 [16:48:43<27:41,  1.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25725/26544 [16:53:44<34:48,  2.55s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25731/26544 [16:54:09<50:28,  3.73s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25732/26544 [16:54:17<1:05:15,  4.82s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25733/26544 [16:54:23<1:12:08,  5.34s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25738/26544 [16:54:43<52:12,  3.89s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25739/26544 [16:54:49<1:03:35,  4.74s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25741/26544 [16:54:59<1:01:00,  4.56s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25742/26544 [16:55:06<1:11:07,  5.32s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25743/26544 [16:55:13<1:18:21,  5.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25744/26544 [16:55:21<1:24:35,  6.34s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25745/26544 [16:55:28<1:28:44,  6.66s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25747/26544 [16:55:38<1:13:17,  5.52s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25748/26544 [16:55:43<1:14:43,  5.63s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25750/26544 [16:55:55<1:13:12,  5.53s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25751/26544 [16:56:02<1:19:41,  6.03s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25752/26544 [16:56:09<1:24:36,  6.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25753/26544 [16:56:17<1:28:52,  6.74s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25754/26544 [16:56:25<1:36:48,  7.35s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25756/26544 [16:56:36<1:20:23,  6.12s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25757/26544 [16:56:44<1:25:42,  6.53s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25759/26544 [16:56:56<1:21:01,  6.19s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25761/26544 [16:57:06<1:08:28,  5.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25767/26544 [16:57:29<47:01,  3.63s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25770/26544 [16:57:42<46:30,  3.61s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25772/26544 [16:57:51<51:14,  3.98s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25773/26544 [16:57:58<1:03:07,  4.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25774/26544 [16:58:06<1:12:20,  5.64s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25775/26544 [16:58:13<1:17:28,  6.04s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25777/26544 [16:58:24<1:12:35,  5.68s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25783/26544 [16:58:46<43:20,  3.42s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25784/26544 [16:58:54<1:00:42,  4.79s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25785/26544 [16:59:02<1:14:18,  5.87s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25786/26544 [16:59:09<1:18:28,  6.21s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25788/26544 [16:59:18<1:06:08,  5.25s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25790/26544 [16:59:28<1:01:41,  4.91s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25791/26544 [16:59:35<1:09:04,  5.50s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25792/26544 [16:59:42<1:14:19,  5.93s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25799/26544 [17:00:05<39:10,  3.16s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25803/26544 [17:00:23<46:14,  3.74s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 97%|█████████▋| 25804/26544 [17:00:31<59:53,  4.86s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 99%|█████████▉| 26234/26544 [17:20:53<16:39,  3.22s/it]  `low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 99%|█████████▉| 26278/26544 [17:23:15<15:00,  3.39s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 99%|█████████▉| 26286/26544 [17:23:46<14:40,  3.41s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      " 99%|█████████▉| 26352/26544 [17:27:32<11:25,  3.57s/it]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "100%|██████████| 26544/26544 [17:37:04<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for audio in tqdm(audio_list):\n",
    "    try:\n",
    "        result = processor.run(audio)\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi xử lý {audio}: {e}\")\n",
    "        continue  # Bỏ qua video lỗi và tiếp tục với video tiếp theo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
